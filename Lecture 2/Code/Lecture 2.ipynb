{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Thresholding using OpenCV\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('Media/Image Before Thresholding.png')\n",
    "\n",
    "# Convert to Grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Simple Thresholding\n",
    "_, threshold = cv2.threshold(gray, 150, 255, cv2.THRESH_OTSU)\n",
    "# Threshold value means if pixel value is greater than 150, it will be assigned 255, else 0\n",
    "# Another Thresholding techniques are cv2.THRESH_BINARY_INV, cv2.THRESH_TRUNC, cv2.THRESH_TOZERO, cv2.THRESH_TOZERO_INV \n",
    "\n",
    "# Adaptive Thresholding: used for images with varying brightness. It divides the image into smaller blocks and then applies thresholding \n",
    "adaptiveThreshold = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2) \n",
    "\n",
    "cv2.imshow('Image Before Thresholding', img)\n",
    "cv2.imshow('Image After Thresholding', threshold)\n",
    "cv2.imshow('Image After Adaptive Thresholding', adaptiveThreshold)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge detection using Canny Edge Detection\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('Media/Image Before Thresholding.png')\n",
    "\n",
    "# Convert to Grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Canny Edge Detection\n",
    "# The first argument is the input image. The second and third arguments are our minVal and maxVal.\n",
    "canny = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "cv2.imshow('Image Before Thresholding', img)\n",
    "cv2.imshow('Image After Canny Edge Detection', canny)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Blurring using OpenCV\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('ImagePath.png')\n",
    "\n",
    "# Blurring\n",
    "Blurred = cv2.blur(img, (7, 7))\n",
    "# Another Blurring techniques are cv2.GaussianBlur, cv2.medianBlur, cv2.bilateralFilter\n",
    "\n",
    "cv2.imshow('Image Before Blurring', img)\n",
    "cv2.imshow('Image After Blurring', Blurred)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('Media/Flower.png')\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 0) # Blurring\n",
    "\n",
    "edged = cv2.Canny(blurred, 170, 255) # Edge Detection\n",
    "\n",
    "# cv2.imshow(\"Original image\", img)\n",
    "# cv2.imshow(\"Edged image\", edged)\n",
    "\n",
    "contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # The two objects returned are contours and hierarchy. We will only use contours.\n",
    "\n",
    "cv2.drawContours(img, contours, -1, (0, 255, 0), 2)\n",
    "# drawContours parameters: image, contours, contourIdx, color, thickness\n",
    "# contourIdx = -1 means all contours are drawn\n",
    "\n",
    "cv2.imshow(\"Edged image\", edged)\n",
    "cv2.imshow(\"contours\", img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imwrite(\"contours.png\", img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Start exploring MediaPipe library**\n",
    "\n",
    "**MediaPipe offers open-source cross-platform, customizable ML solutions for live and streaming media.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with install the library\n",
    "# You need Python version < 3.11\n",
    "%pip install mediapipe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify a single Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The image classifier was trained using 'ImageNet' Dataset to recognize 1,000 classes**\n",
    "\n",
    "For more information about the available classifiers check [MediaPipe Image Classification Task guide](https://developers.google.com/mediapipe/solutions/vision/image_classifier/index#efficientnet-lite0_model_recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "ImageClassifier = mp.tasks.vision.ImageClassifier\n",
    "ImageClassifierOptions = mp.tasks.vision.ImageClassifierOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = ImageClassifierOptions(\n",
    "    base_options=BaseOptions(model_asset_path='Models/efficientnet_lite0.tflite'), # Path to the model file\n",
    "    max_results=5, # Maximum number of results to return\n",
    "    running_mode=VisionRunningMode.IMAGE, # Running mode 'Image' or 'Video' or 'Stream'\n",
    "    category_allowlist = ['sports car', 'pizza', 'pomegranate', 'tiger cat'] # List of categories to allow\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"Media/Ferrari.jpg\") # Load the image\n",
    "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img) # Create a MediaPipe Image object from the OpenCV image\n",
    "\n",
    "with ImageClassifier.create_from_options(options) as classifier: # Create the ImageClassifier instance\n",
    "    classification_result = classifier.classify(mp_image) # Classify the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: sports car\n",
      "Score: 0.83984375\n"
     ]
    }
   ],
   "source": [
    "# classification_result contains the classification results.\n",
    "print(f\"Category: {classification_result.classifications[0].categories[0].category_name}\") # Get the category of the highest classification result\n",
    "print(f\"Score: {classification_result.classifications[0].categories[0].score}\") # Get the score of the highest classification result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection with MediaPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On a single Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceDetector = mp.tasks.vision.FaceDetector\n",
    "FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Initialize a face detector instance:\n",
    "options = FaceDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path='Models/blaze_face_short_range.tflite'),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_suppression_threshold=0.3)\n",
    "# Create instance of the face detector and load an image\n",
    "with FaceDetector.create_from_options(options) as detector:\n",
    "    img = cv2.imread(\"Media/Person.jpg\")\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "    face_detector_result = detector.detect(mp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetectionResult(detections=[Detection(bounding_box=BoundingBox(origin_x=333, origin_y=129, width=161, height=161), categories=[Category(index=0, score=0.8582574129104614, display_name=None, category_name=None)], keypoints=[NormalizedKeypoint(x=0.458046555519104, y=0.3762951195240021, label='', score=0.0), NormalizedKeypoint(x=0.5414471626281738, y=0.37260696291923523, label='', score=0.0), NormalizedKeypoint(x=0.5043702125549316, y=0.4522835314273834, label='', score=0.0), NormalizedKeypoint(x=0.504838764667511, y=0.5268869400024414, label='', score=0.0), NormalizedKeypoint(x=0.4082053303718567, y=0.4142933785915375, label='', score=0.0), NormalizedKeypoint(x=0.5878703594207764, y=0.400863379240036, label='', score=0.0)])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detector_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bounding box of the first face:\n",
    "x = face_detector_result.detections[0].bounding_box.origin_x\n",
    "y = face_detector_result.detections[0].bounding_box.origin_y\n",
    "height = face_detector_result.detections[0].bounding_box.height\n",
    "width = face_detector_result.detections[0].bounding_box.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the bounding box on the image:\n",
    "cv2.rectangle(img, (int(x), int(y)), (int(x + width), int(y + height)), (0, 255, 0), 2) # The first coordinate is the top-left corner of the bounding box, \n",
    "                                                                                        # and the second coordinate is the bottom-right corner of the bounding box\n",
    "cv2.imshow(\"Face Detection\", img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Face Detection on a video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceDetector = mp.tasks.vision.FaceDetector\n",
    "FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a face detector instance with the video mode:\n",
    "options = FaceDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path='Models/blaze_face_short_range.tflite'),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_suppression_threshold=0.3)\n",
    "with FaceDetector.create_from_options(options) as detector:\n",
    "    vid = cv2.VideoCapture(\"Media/video_1.mp4\") # Load the video\n",
    "\n",
    "    fps = vid.get(cv2.CAP_PROP_FPS) # Get the frame rate of the video\n",
    "\n",
    "    while True:\n",
    "        success, frame = vid.read() # Read a frame from the video\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame) # Create a MediaPipe Image object from the OpenCV image\n",
    "\n",
    "        face_detector_result = detector.detect(mp_image) \n",
    "\n",
    "        if face_detector_result.detections: # If faces are detected\n",
    "            for detection in face_detector_result.detections: # Loop through the detected faces \n",
    "                # Get the coordinates of the face:\n",
    "                x = detection.bounding_box.origin_x\n",
    "                y = detection.bounding_box.origin_y\n",
    "                height = detection.bounding_box.height\n",
    "                width = detection.bounding_box.width\n",
    "                cv2.rectangle(frame, (int(x), int(y)), (int(x + width), int(y + height)), (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Face Detection\", frame)\n",
    "        if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytesseract via this link: \n",
    "# https://github.com/UB-Mannheim/tesseract/wiki\n",
    "# Add the path to the tesseract executable to the system environment variable PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tesseract \n",
    "%pip install pytesseract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Text Extraction with pytesseract \n",
    "import pytesseract\n",
    "import cv2\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe' # Path to the tesseract executable\n",
    "\n",
    "img = cv2.imread(\"Media/Book Paragraph.jpg\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "text = pytesseract.image_to_string(gray) \n",
    "\n",
    "print(text)\n",
    "\n",
    "cv2.imshow(\"Adaptive Threshold\", gray)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
